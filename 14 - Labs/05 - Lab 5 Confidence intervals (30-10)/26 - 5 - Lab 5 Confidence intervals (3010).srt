
1
00:00:00,970 --> 00:00:03,460
Welcome to Statistics one, lab five.

2
00:00:03,460 --> 00:00:08,850
The goals in today's lab will be, will
repeat a lot of the steps that we've

3
00:00:08,850 --> 00:00:11,260
done in previous labs, so we'll read a

4
00:00:11,260 --> 00:00:14,920
data file into R, we'll print some summary
statistics.

5
00:00:14,920 --> 00:00:20,750
But the emphasis this week is on null
hypothesis significance testing.

6
00:00:20,750 --> 00:00:26,360
As well as standard error, confidence
intervals and model comparison.

7
00:00:26,360 --> 00:00:28,930
So we'll do correlation and regression
analyses

8
00:00:28,930 --> 00:00:34,530
again but emphasizing significance tests
and the problems

9
00:00:34,530 --> 00:00:37,220
with significance tests and how to
supplement them

10
00:00:37,220 --> 00:00:40,130
with things like confidence intervals and
model comparisons.

11
00:00:41,160 --> 00:00:42,950
And I'm going to use the same example that

12
00:00:42,950 --> 00:00:47,205
I used last week the correlational study
investigating

13
00:00:47,205 --> 00:00:48,860
[SOUND]

14
00:00:48,860 --> 00:00:51,530
the predictors of physical endurance in

15
00:00:51,530 --> 00:00:55,020
adults the outcome variable is physical
endurance.

16
00:00:55,020 --> 00:01:00,060
The predictors are age and number of years
actively engaged in exercise.

17
00:01:00,060 --> 00:01:03,770
The initial analyses are the same as last

18
00:01:03,770 --> 00:01:06,360
week, so we're going to assume a sample
size of

19
00:01:06,360 --> 00:01:09,740
200, but then to show you the importance

20
00:01:09,740 --> 00:01:13,960
and the impact of sample size I'll reduce
that,

21
00:01:13,960 --> 00:01:18,350
I'll take a random subset of the 200, just
20.

22
00:01:18,350 --> 00:01:21,270
And then we'll rerun the analyses.

23
00:01:21,270 --> 00:01:23,980
again, I've commented out all this
beginning stuff.

24
00:01:23,980 --> 00:01:27,230
So just make sure you're in the right
working directory.

25
00:01:27,230 --> 00:01:29,210
you might need to install a new package
here.

26
00:01:29,210 --> 00:01:34,130
We're going to use ggplot 2, which we'll
use a lot going forward in the course.

27
00:01:34,130 --> 00:01:39,620
It's a, it's, it's sort of the premier
graphics package in R.

28
00:01:39,620 --> 00:01:44,600
so go ahead and install that package.
Now I'm going to start right here.

29
00:01:44,600 --> 00:01:49,590
I'm going to load my packages with the
library function.

30
00:01:49,590 --> 00:01:53,520
I'll read in the data I don't need to look
at it because we've seen

31
00:01:53,520 --> 00:01:55,674
this before but then I'll run the

32
00:01:55,674 --> 00:01:58,475
describe function just to get the summary
statistics.

33
00:01:58,475 --> 00:02:01,060
[SOUND]

34
00:02:01,060 --> 00:02:04,730
So here are the summary statistics.
And this is what we saw last week.

35
00:02:04,730 --> 00:02:09,530
We have 200 subjects with a, a mean age of
about 49.

36
00:02:09,530 --> 00:02:11,800
Active years about ten and a half.

37
00:02:11,800 --> 00:02:17,250
Endurance, remember was this scale that
went from zero to 60, an arbitrary scale.

38
00:02:17,250 --> 00:02:20,260
But we did all that last week.
So let's quickly go back.

39
00:02:20,260 --> 00:02:20,970
To the script.

40
00:02:22,260 --> 00:02:26,500
In this lab I'm going to illustrate some
of the calculations of these values

41
00:02:26,500 --> 00:02:28,060
because we're not doing a lot of hand

42
00:02:28,060 --> 00:02:30,980
calculations which is different from my
live class.

43
00:02:33,020 --> 00:02:35,840
so I just want to show you where these
numbers are coming from.

44
00:02:35,840 --> 00:02:42,880
So just in that describe output you saw
standard error.

45
00:02:42,880 --> 00:02:47,670
So standard error, if you look at this
line here in my, in my script.

46
00:02:47,670 --> 00:02:50,670
Standard error's just the standard
deviation divided

47
00:02:50,670 --> 00:02:53,240
by the square root of sample size.

48
00:02:53,240 --> 00:02:58,200
Remember standard error is just how much
sampling error on average do we expect?

49
00:02:59,950 --> 00:03:01,080
And this is the formula.

50
00:03:01,080 --> 00:03:04,285
So standard error is standard deviation
over the square root of n.

51
00:03:05,300 --> 00:03:08,570
So what I can do in R is I can take

52
00:03:08,570 --> 00:03:12,090
that described function and I can save it
into a table.

53
00:03:12,090 --> 00:03:14,030
I'll call it table 1.

54
00:03:14,030 --> 00:03:18,670
And if I do that, let's execute those two
lines of code.

55
00:03:18,670 --> 00:03:20,130
Then, I just have table 1.

56
00:03:20,130 --> 00:03:24,680
It's the same exact thing as I just, as I
just got from the describe function.

57
00:03:24,680 --> 00:03:24,980
But now

58
00:03:24,980 --> 00:03:29,020
I can access the values inside that table,
which becomes

59
00:03:29,020 --> 00:03:32,300
really handy and we'll do this a lot going
forward.

60
00:03:32,300 --> 00:03:35,360
So for example, if I want the standard
deviation

61
00:03:35,360 --> 00:03:38,800
for the age variable, then I could just go

62
00:03:38,800 --> 00:03:42,160
into the table and go to row 2, column

63
00:03:42,160 --> 00:03:46,100
4, because that's where standard deviation
for age is.

64
00:03:46,100 --> 00:03:49,820
And if I want the sample size, I can go to
row 2, column 2.

65
00:03:49,820 --> 00:03:50,200
So let's

66
00:03:50,200 --> 00:03:51,785
just execute those.

67
00:03:51,785 --> 00:03:55,990
[SOUND]

68
00:03:55,990 --> 00:04:02,150
So the standard deviation for age is in
this table, and it's in row 2, column 4.

69
00:04:02,150 --> 00:04:08,811
So if I go to row 2, there's age.
And column 4, 1, 2, 3, 4.

70
00:04:08,811 --> 00:04:14,676
There's standard deviation.
Okay, so it's, it's going to return 10.48.

71
00:04:14,676 --> 00:04:17,172
And the sample size for age well its the

72
00:04:17,172 --> 00:04:20,950
same for all variables but it's, it's
table 1.

73
00:04:20,950 --> 00:04:23,340
Row 2 column 2.
So row 2.

74
00:04:23,340 --> 00:04:26,430
Again, age.
Column 2, 200.

75
00:04:26,430 --> 00:04:31,460
The reason I'm showing you that is we
could, we don't need

76
00:04:31,460 --> 00:04:34,260
to do this because R gives it to you in
the describe function.

77
00:04:34,260 --> 00:04:40,790
But you could calculate the standard error
of age based on those values.

78
00:04:40,790 --> 00:04:43,980
So I could just calculate the standard
error

79
00:04:43,980 --> 00:04:46,810
of age based on the standard deviation and

80
00:04:46,810 --> 00:04:48,309
the square root of the sample size.

81
00:04:50,310 --> 00:04:56,310
So if I execute that line of code, I get
0.74 for the standard error

82
00:04:56,310 --> 00:05:00,490
of age, and if we go back up in the table,
yep, there it is.

83
00:05:00,490 --> 00:05:03,220
So we did the calculation correctly.

84
00:05:03,220 --> 00:05:08,990
Just to be really sure, I could test it
and see if my calculation equals

85
00:05:08,990 --> 00:05:13,730
Table 1, row 2, column 13, that's where
standard error was.

86
00:05:14,788 --> 00:05:15,892
so if I execute

87
00:05:15,892 --> 00:05:18,250
that, I get true.
Okay?

88
00:05:18,250 --> 00:05:19,470
So we did it right.

89
00:05:19,470 --> 00:05:21,240
The point of that is to illustrate that
you

90
00:05:21,240 --> 00:05:25,010
can save output in R into tables, like I
did

91
00:05:25,010 --> 00:05:27,180
here, save it into table 1, and then you

92
00:05:27,180 --> 00:05:33,080
can access the values in that table to do
calculations.

93
00:05:33,080 --> 00:05:35,460
And that's going to be handy, going
forward.

94
00:05:35,460 --> 00:05:40,370
Okay, so let's go back to the main topic,
correlation regression.

95
00:05:40,370 --> 00:05:41,420
we ran

96
00:05:41,420 --> 00:05:47,420
this correlation analysis last week, but
what we, what we didn't do is, core.test.

97
00:05:47,420 --> 00:05:51,570
So, we can run a null hypothesis
significance test on

98
00:05:51,570 --> 00:05:57,070
each of these correlations, to see if they
are statistically significant.

99
00:05:57,070 --> 00:06:00,930
In other words, the null hypothesis is
that the correlation coefficient is 0.

100
00:06:00,930 --> 00:06:06,430
And this will give us a p value.
Which stands for the probability of

101
00:06:06,430 --> 00:06:11,910
obtaining this correlation given the
assumption that the, null is true.

102
00:06:11,910 --> 00:06:14,819
So let's see what we get from these.

103
00:06:14,819 --> 00:06:16,346
[SOUND]

104
00:06:16,346 --> 00:06:18,770
Let me go back up a little bit.

105
00:06:18,770 --> 00:06:21,250
So, remember these are the same data from
last

106
00:06:21,250 --> 00:06:25,610
week, so we had, like, correlations of
0.33 between age

107
00:06:25,610 --> 00:06:28,420
and active years, and between endurance
and active years,

108
00:06:28,420 --> 00:06:31,090
but then not much going on between age and
endurance.

109
00:06:32,470 --> 00:06:35,000
Here's the output for, for core.test.

110
00:06:35,000 --> 00:06:37,850
It gives the Pearson product moment
correlation.

111
00:06:37,850 --> 00:06:40,690
Here is the correlation, 0.33.
.

112
00:06:40,690 --> 00:06:41,830
And here's

113
00:06:41,830 --> 00:06:43,950
the p values.
It's a very low p value.

114
00:06:43,950 --> 00:06:44,920
That's significant.

115
00:06:46,330 --> 00:06:53,950
same is true for the correlation between
endurance and active years, 0.33.

116
00:06:53,950 --> 00:06:54,650
Low p-value.

117
00:06:54,650 --> 00:07:00,830
But then for endurance and age that's a
correlation of

118
00:07:00,830 --> 00:07:06,100
negative 08 and that's a P value of 0.23,
so there we would have to retain the null.

119
00:07:06,100 --> 00:07:07,840
That's not a statistically

120
00:07:07,840 --> 00:07:09,120
significant correlation.

121
00:07:11,960 --> 00:07:15,780
Okay, I'm going to save those correlations
into a table.

122
00:07:15,780 --> 00:07:18,440
Table 2, because we're going to use them

123
00:07:18,440 --> 00:07:22,990
to illustrate some calculations in the
regression analysis.

124
00:07:22,990 --> 00:07:25,294
So let me just save that, table 2, go

125
00:07:25,294 --> 00:07:28,800
back into the console, looks like it did
it fine.

126
00:07:28,800 --> 00:07:29,690
Great.

127
00:07:29,690 --> 00:07:31,820
Okay, let's move on to the regression
analyses.

128
00:07:31,820 --> 00:07:36,890
I'll start with unstandardized an
unstandardized regression analysis,

129
00:07:36,890 --> 00:07:38,600
just a simple regression.

130
00:07:38,600 --> 00:07:41,620
Again, we did this last week, so I'm going
to try and go through this quickly.

131
00:07:44,030 --> 00:07:47,020
So here is that first model predicting
endurance

132
00:07:47,020 --> 00:07:49,700
from age, and of course, that's not
significant.

133
00:07:50,990 --> 00:07:55,170
And if you notice, the p-value here is
0.232 for that regression coefficient.

134
00:07:55,170 --> 00:07:59,310
It's the same exact p-value as up here for
the correlation coefficient.

135
00:07:59,310 --> 00:08:00,680
All right, that's true because we're just

136
00:08:00,680 --> 00:08:03,320
doing a simple regression with one
predictor.

137
00:08:05,570 --> 00:08:08,930
But now let's illustrate some calculations
again.

138
00:08:08,930 --> 00:08:12,450
So, how would we calculate that regression
coefficient?

139
00:08:12,450 --> 00:08:16,250
Well, I talked about it in lecture where
we, I talked about calculating the

140
00:08:16,250 --> 00:08:20,250
sums of squares residual, and the sums of
squares for the model, and so on.

141
00:08:22,018 --> 00:08:23,840
but another formula I gave you for

142
00:08:23,840 --> 00:08:29,020
the regression coefficient is this formula
right here.

143
00:08:29,020 --> 00:08:31,210
The regression coefficient can be

144
00:08:31,210 --> 00:08:35,440
calculated by taking the correlation
coefficient and multiplying by the

145
00:08:35,440 --> 00:08:39,360
standard deviation of y over the standard
deviation of x.

146
00:08:39,360 --> 00:08:42,780
Well, I have all these values now in
tables,

147
00:08:42,780 --> 00:08:46,410
right, so in table 2, that's my
correlation matrix.

148
00:08:46,410 --> 00:08:50,020
I'm looking at the relationship between
age and endurance.

149
00:08:50,020 --> 00:08:53,110
So, let's go back into the console here.

150
00:08:56,210 --> 00:09:04,290
If I look at the correlation matrix, it's
going to be row 3, column 1 to get this

151
00:09:04,290 --> 00:09:11,670
correlation coefficient for this model.
That's why I get row 3, column 1.

152
00:09:11,670 --> 00:09:16,770
So the model's regression coefficient,
model 1's B,

153
00:09:16,770 --> 00:09:21,140
is the correlation coefficient, which is
in table 2,

154
00:09:21,140 --> 00:09:25,110
row 3, column 1, times the standard
deviation

155
00:09:25,110 --> 00:09:27,810
of y, which is in the first table.

156
00:09:27,810 --> 00:09:31,230
Row 4, column 4, and the standard devia-,
deviation

157
00:09:31,230 --> 00:09:34,770
of X, which is in row 2, column 4.

158
00:09:34,770 --> 00:09:38,670
So let me just execute all of that.

159
00:09:38,670 --> 00:09:39,340
[SOUND]

160
00:09:39,340 --> 00:09:44,700
And what we see here is we have a, we
calculated a

161
00:09:44,700 --> 00:09:50,210
regression coefficient of negative 0.08.

162
00:09:50,210 --> 00:09:55,242
If you go back up into our output from

163
00:09:55,242 --> 00:09:59,980
the LM, that's exactly what we get.

164
00:09:59,980 --> 00:10:03,090
Little bit of rounding error but we did it
right.

165
00:10:03,090 --> 00:10:05,070
So here's just illustrating

166
00:10:05,070 --> 00:10:11,568
how you can calculate these values.
from the basic formulas that I gave you

167
00:10:11,568 --> 00:10:17,766
in lecture rather than just relying on the
LM function to give it, to give it to you.

168
00:10:17,766 --> 00:10:19,550
just helps to sort of understand this at
first.

169
00:10:19,550 --> 00:10:25,650
Of course, you'd never do this piece
again, this is just to help you learn the

170
00:10:25,650 --> 00:10:29,960
concepts and learn the math underlying.
linear regression.

171
00:10:31,000 --> 00:10:36,600
Okay, so now let's look at how the
standard error is calculated.

172
00:10:36,600 --> 00:10:41,630
So standard error of a regression
coefficient is, it's a little bit

173
00:10:41,630 --> 00:10:45,590
more difficult, the formula, it's not
quite as intuitive, but here it is.

174
00:10:45,590 --> 00:10:50,800
It's the square root of the sums of
squares residual, over and minus 2,

175
00:10:50,800 --> 00:10:56,080
divided by the sums of squares for X.
So, VertNet's sort of in notation

176
00:10:56,080 --> 00:10:57,450
form here.

177
00:10:57,450 --> 00:11:01,610
Standard error for the regression
coefficient is the square root of sums of

178
00:11:01,610 --> 00:11:06,210
squares residual over N minus two, divided
by sums of squares for X.

179
00:11:07,280 --> 00:11:11,480
I'm going to create one more table here
using the ANOVA function.

180
00:11:12,630 --> 00:11:15,450
And that's going to give me the sums of
squares residual.

181
00:11:15,450 --> 00:11:17,640
Of'course we could calculate it by hand
right.

182
00:11:17,640 --> 00:11:21,580
We could go through all 200 subjects and

183
00:11:21,580 --> 00:11:25,160
get their predicted score and their actual
score and

184
00:11:25,160 --> 00:11:27,980
get the residual and square it and sum it.

185
00:11:28,980 --> 00:11:32,580
but that would be pretty tedious, and this
is going to be a long lab anyway.

186
00:11:32,580 --> 00:11:35,110
So let's just use that ANOVA function.

187
00:11:37,690 --> 00:11:44,370
So what the ANOVA function returns are
these sums of squares for the model.

188
00:11:44,370 --> 00:11:47,910
So if you think back to lecture, where I
talked about sums of squares for

189
00:11:47,910 --> 00:11:53,320
model and sums of squares for residual,
this is sums of squares for the model.

190
00:11:53,320 --> 00:11:55,720
This is sums of squares, residual.

191
00:11:55,720 --> 00:11:59,110
So we're going to use this value to
calculate standard error.

192
00:12:01,110 --> 00:12:02,690
So, sums of the squares

193
00:12:02,690 --> 00:12:05,730
residual is in table 3, row 2, column 2.

194
00:12:05,730 --> 00:12:08,715
Just to make sure we did that right, let's
execute that.

195
00:12:08,715 --> 00:12:12,310
[SOUND]

196
00:12:12,310 --> 00:12:14,860
Right.
So there's the table.

197
00:12:14,860 --> 00:12:17,080
Row two, column two, 23,000.

198
00:12:17,080 --> 00:12:22,191
I now want to divide that

199
00:12:22,191 --> 00:12:27,800
by n minus 2.
Well, that's degrees of freedom.

200
00:12:27,800 --> 00:12:29,420
We'll ha, we'll talk a lot about degrees

201
00:12:29,420 --> 00:12:32,250
of freedom when we cover analysis of
variance.

202
00:12:32,250 --> 00:12:35,150
but for now, in this example it's just n
minus 2.

203
00:12:35,150 --> 00:12:37,120
So, that was in the table.
It's,

204
00:12:37,120 --> 00:12:40,750
it's in the, row 2, column 1, so let me
just grab

205
00:12:40,750 --> 00:12:45,220
that.
So if you look at

206
00:12:45,220 --> 00:12:49,650
this table, row 2, column 1, remember
there are

207
00:12:49,650 --> 00:12:54,168
200 subjects, so 200 minus 2 is 198, and
there I have that value.

208
00:12:54,168 --> 00:13:02,480
The sums of squares for X, the entire
value, variable X Is the sums of squares

209
00:13:02,480 --> 00:13:05,760
for the model, and the sums of squares of
the residual.

210
00:13:05,760 --> 00:13:11,860
So I can go back into the table in column
2.

211
00:13:11,860 --> 00:13:14,140
And just take row 1 and row 2 and add

212
00:13:14,140 --> 00:13:17,746
them together, and that'll give me sums of
squares x.

213
00:13:17,746 --> 00:13:18,910
So, lemme do that.

214
00:13:21,940 --> 00:13:24,530
So now I have sums of squares x.
It's just the sum of, oops.

215
00:13:24,530 --> 00:13:28,890
Just the sum of this column here.

216
00:13:33,080 --> 00:13:37,170
So now we can calculate the standard error
of the regression coefficient.

217
00:13:37,170 --> 00:13:40,650
It's the square root of the sums of
squares, rigid, residual which

218
00:13:40,650 --> 00:13:43,960
we got from table 3 which we got from
running the ANOVA function.

219
00:13:45,100 --> 00:13:47,420
Degrees of freedom also came from that
ANOVA

220
00:13:47,420 --> 00:13:49,710
table but we could have just plugged it
in.

221
00:13:49,710 --> 00:13:51,470
It's the total number of subjects minus 2.

222
00:13:51,470 --> 00:13:58,370
And sums of squares x, again, we got from
the ANOVA table, it's just the

223
00:13:58,370 --> 00:14:02,010
sums of squares for the model plus sums of
squares residual.

224
00:14:02,010 --> 00:14:03,745
So now, if I do that.

225
00:14:03,745 --> 00:14:07,230
[SOUND]

226
00:14:07,230 --> 00:14:12,320
The standard error for the regression
coefficient according to our calculations

227
00:14:12,320 --> 00:14:20,510
is 0.07 if we go back up to where we ran
the LM function.

228
00:14:20,510 --> 00:14:22,960
You see the standard error is 0.07.
Great.

229
00:14:22,960 --> 00:14:27,180
So we did it right, and it matches up.

230
00:14:27,180 --> 00:14:30,320
Again, going forward when you do multiple
regression,

231
00:14:30,320 --> 00:14:32,935
you're just going to rely on this LM call
and

232
00:14:32,935 --> 00:14:36,800
you're going to rely on r to do the
calculations for you.

233
00:14:36,800 --> 00:14:40,020
But I think it's nice to go through this
once when

234
00:14:40,020 --> 00:14:42,430
you're first learning it to see where the
numbers are coming from.

235
00:14:44,210 --> 00:14:48,840
Okay, so, now let's move to confidence
intervals.

236
00:14:48,840 --> 00:14:51,780
It's really easy to get confidence
intervals in r.

237
00:14:51,780 --> 00:14:54,640
So if I want the 95 percent confidence
interval

238
00:14:54,640 --> 00:14:58,850
around that regression coefficient, all I
need to do is

239
00:14:58,850 --> 00:15:04,160
use the function confint, So confint
around model 1.

240
00:15:04,160 --> 00:15:09,120
You can use the confint function around
any fitted model.

241
00:15:09,120 --> 00:15:10,020
So if I do that,

242
00:15:13,320 --> 00:15:17,280
it returns the 95% confidence interval for

243
00:15:17,280 --> 00:15:19,700
the regression, the regression constant,
but also

244
00:15:19,700 --> 00:15:21,170
the regression coefficient, and that's
what we

245
00:15:21,170 --> 00:15:24,750
really are concerned with, is this
interval.

246
00:15:24,750 --> 00:15:27,590
Alright so it goes from negative 23 to
positive 0.05.

247
00:15:27,590 --> 00:15:33,150
The fact that it spans 0 indicates to you
that it's not going to

248
00:15:33,150 --> 00:15:38,645
be a significant regression coefficient.
Okay, because we are in,

249
00:15:38,645 --> 00:15:43,530
in the 95% confidence interval is included
0,

250
00:15:43,530 --> 00:15:46,680
no relationship so that's a sign right
there

251
00:15:46,680 --> 00:15:50,750
that this is that we're going to retail
the null and that this is not significant.

252
00:15:54,070 --> 00:15:57,990
We could also illustrate the calculation
of that confidence interval.

253
00:15:57,990 --> 00:16:01,400
So, the upper value of a con, confidence

254
00:16:01,400 --> 00:16:04,490
interval, to get the higher value, you
take

255
00:16:04,490 --> 00:16:09,460
the regression coefficient and you add a
product

256
00:16:09,460 --> 00:16:12,410
term, which is a function of standard
error.

257
00:16:12,410 --> 00:16:16,270
It's just standard error times some,
critical t

258
00:16:16,270 --> 00:16:20,100
value which is based on degrees of freedom
and

259
00:16:20,100 --> 00:16:23,890
the degree of confidence.
It turns out there's a really nifty way

260
00:16:23,890 --> 00:16:29,300
you can get that in R using this QT
function

261
00:16:29,300 --> 00:16:34,570
so.
I combine 0.025 and 0.975, because

262
00:16:34,570 --> 00:16:40,900
that puts 2.5% of the distribution on the
lower end

263
00:16:40,900 --> 00:16:46,930
of the sampling distribution, and 2.5% on
the higher end.

264
00:16:46,930 --> 00:16:52,993
And then my degrees of freedom is 198.
So, right here is where I'm telling it

265
00:16:52,993 --> 00:16:59,400
that I want 95%, here is where I'm telling
it what degrees of freedom are.

266
00:16:59,400 --> 00:17:00,890
And remember I have to, I have to

267
00:17:00,890 --> 00:17:04,170
indicate degrees of freedom because
remember there's a

268
00:17:04,170 --> 00:17:08,210
family of t distributions and they differ
slightly

269
00:17:08,210 --> 00:17:10,970
according to sample size and degrees of
freedom.

270
00:17:10,970 --> 00:17:12,290
So this tcrit

271
00:17:12,290 --> 00:17:17,280
value depends on not just the degree of
confidence but also

272
00:17:17,280 --> 00:17:22,466
degrees of freedom.
So let me run that, and,

273
00:17:22,466 --> 00:17:27,660
that tcrit value is about 1.97.

274
00:17:27,660 --> 00:17:30,910
In a perfect Gaussian distribution a, a
perfect

275
00:17:30,910 --> 00:17:36,470
normal distribution the z distribution
that corresponds to 1.96.

276
00:17:36,470 --> 00:17:41,220
So we're pretty close because this is a
large sample size.

277
00:17:41,220 --> 00:17:42,450
it's just a little wider.

278
00:17:44,120 --> 00:17:49,460
So now I could calculate the interval as
the regression coefficient.

279
00:17:49,460 --> 00:17:55,250
Which was negative 0.08 plus T crit times
standard error.

280
00:17:58,840 --> 00:18:03,350
And I get my interval.
Negative 22 to positive 0.05.

281
00:18:03,350 --> 00:18:07,830
That's the same thing we got when we ran
the conf int function.

282
00:18:09,220 --> 00:18:11,084
Little bit of rounding error.

283
00:18:11,084 --> 00:18:18,190
again we'll never do this piece again
we'll just use the conf int function but.

284
00:18:18,190 --> 00:18:20,260
Again, I wanted to illustrate where these
numbers are

285
00:18:20,260 --> 00:18:22,670
coming from, and that's, that's the
formula right there.

286
00:18:23,780 --> 00:18:26,490
Okay, let's go back into the script.

287
00:18:28,400 --> 00:18:29,860
now we're just going to look at the

288
00:18:29,860 --> 00:18:31,770
scatter plot again, but now we're going to
make

289
00:18:31,770 --> 00:18:35,770
use of these fancier graphics that, exist

290
00:18:35,770 --> 00:18:40,300
in the ggplot package, and they're really
cool.

291
00:18:40,300 --> 00:18:43,060
And, what I like about this scatter plot,
and I, I really

292
00:18:43,060 --> 00:18:49,160
encourage my colleagues and my, my
collaborators to do this, is to

293
00:18:49,160 --> 00:18:52,600
plot confidence intervals around a
regression line.

294
00:18:52,600 --> 00:18:58,730
Because remember a regression coefficient
is just a point estimate.

295
00:18:58,730 --> 00:19:02,000
And it's susceptible to sampling error.

296
00:19:02,000 --> 00:19:08,080
So we should be explicit about that in our
graphical representation of

297
00:19:08,080 --> 00:19:14,380
the correlation, and of the scatter plot.
Ggplot allows us to do that.

298
00:19:14,380 --> 00:19:19,460
So let's see what it looks like.
And there we go.

299
00:19:19,460 --> 00:19:20,330
That is very cool.

300
00:19:21,950 --> 00:19:26,980
so I plotted age on the x axis, endurance
on the y axis.

301
00:19:28,300 --> 00:19:32,720
Remember, the dots are all just individual
participants.

302
00:19:32,720 --> 00:19:34,810
And they're sort of all over the place,
because

303
00:19:34,810 --> 00:19:36,890
there's, there's not much of a correlation
here, right?

304
00:19:36,890 --> 00:19:39,800
It was like, negative 0.08 or something,
so there's

305
00:19:39,800 --> 00:19:45,840
a slight negative slope.
But notice this shaded region around it.

306
00:19:45,840 --> 00:19:48,400
That represents the 95% confidence

307
00:19:48,400 --> 00:19:51,850
interval around that regression
coefficient.

308
00:19:51,850 --> 00:19:56,350
And that's a really nice thing to do when
plotting scatter plots.

309
00:19:56,350 --> 00:20:02,010
And unfortunately it's just not done
enough at least in the social sciences

310
00:20:02,010 --> 00:20:05,320
but hopefully we can, we can encourage
people to do this more often.

311
00:20:06,350 --> 00:20:11,110
I think it looks pretty cool.
Okay, so now let's look at model 2.

312
00:20:11,110 --> 00:20:14,310
I'm not going to go through all these
illustrations

313
00:20:14,310 --> 00:20:16,490
of calculations again, because that takes
a while.

314
00:20:17,498 --> 00:20:19,044
and we did this last week.

315
00:20:19,044 --> 00:20:26,610
so let's just print the, the output of the
model and the confidence intervals.

316
00:20:29,440 --> 00:20:34,465
So here we're predicting endurance from
active years, and the

317
00:20:34,465 --> 00:20:39,650
t-value for that regression coefficient is
4.8, and that's significant.

318
00:20:39,650 --> 00:20:39,820
Right.

319
00:20:39,820 --> 00:20:43,220
We're now explaining about 11% of the
variance.

320
00:20:43,220 --> 00:20:49,779
And if you look at the confidence interval
it goes from about 0.44 to 1.06.

321
00:20:49,779 --> 00:20:54,180
So 0 is nowhere near that 95% confidence
interval.

322
00:20:54,180 --> 00:20:56,770
Another sign that this is a significant
predictor.

323
00:21:00,060 --> 00:21:03,760
Let's look at our cool new scatter plot
with the confidence interval on it.

324
00:21:05,180 --> 00:21:05,640
Nice.

325
00:21:05,640 --> 00:21:06,400
There we go.

326
00:21:07,820 --> 00:21:12,140
we got a nice strong positive correlation.

327
00:21:12,140 --> 00:21:15,830
And we put in the confidence interval
around the regression line.

328
00:21:17,990 --> 00:21:21,238
Le'ts quickly just move through model 3.

329
00:21:21,238 --> 00:21:24,215
we, again, we did this last week, so I'm
just

330
00:21:24,215 --> 00:21:26,725
going to zip through it and just show you
the plot.

331
00:21:26,725 --> 00:21:29,900
[SOUND]

332
00:21:29,900 --> 00:21:30,730
There we go.

333
00:21:30,730 --> 00:21:34,336
The, the slope is a little bit steeper
here.

334
00:21:34,336 --> 00:21:40,790
as we go back into the output we're now
explaining about 15% of the variance.

335
00:21:40,790 --> 00:21:44,320
So we're doing a little better by putting
both

336
00:21:44,320 --> 00:21:47,600
predictors in than by just putting active
years in.

337
00:21:49,190 --> 00:21:55,700
And the final thing we can do is conduct a
model comparison,

338
00:21:55,700 --> 00:21:59,560
an actual null hypothesis significance
test.

339
00:21:59,560 --> 00:22:05,910
To determine whether model 3 is
significantly better than model 2.

340
00:22:05,910 --> 00:22:09,080
And it's real simple, we use this ANOVA
function again.

341
00:22:09,080 --> 00:22:14,239
So we just do ANOVA model 2 model 3, and
that will give me a p-value indicating

342
00:22:14,239 --> 00:22:17,858
whether I should reject or retain the null
hypothesis,

343
00:22:17,858 --> 00:22:20,784
where the null hypothesis is that model 2
and

344
00:22:20,784 --> 00:22:22,090
model 3.

345
00:22:22,090 --> 00:22:27,000
Are equivalent in terms of how well they
explain the outcome variable.

346
00:22:30,180 --> 00:22:37,580
So here's the output from that model
comparison, line of code.

347
00:22:37,580 --> 00:22:40,730
And here it gives us an F statistic, which

348
00:22:40,730 --> 00:22:42,350
we're going to talk a lot more about when
we

349
00:22:42,350 --> 00:22:44,730
get to analysis of variance, so just bear
with

350
00:22:44,730 --> 00:22:46,940
me here, I'm going to gloss over some of
the details.

351
00:22:47,970 --> 00:22:52,220
but it's, it's a lot like the T statistic
it,

352
00:22:52,220 --> 00:22:55,405
you get a certain value, and then it has a
corresponding

353
00:22:55,405 --> 00:22:57,890
p-value based on a sampling distribution.

354
00:22:57,890 --> 00:23:00,420
Again, we'll talk about this more in-depth
when we cover ANOVA.

355
00:23:00,420 --> 00:23:05,664
But the key point here is that the p-value
is less than 0.05, so we can say

356
00:23:05,664 --> 00:23:11,420
that the difference between model 2 and
model 3 is statistically significant.

357
00:23:11,420 --> 00:23:13,390
So model 3 is doing better.

358
00:23:13,390 --> 00:23:20,730
Also notice that this p-value, is exactly
the same p-value as the one

359
00:23:20,730 --> 00:23:24,960
for age, when age was added in with active
years.

360
00:23:24,960 --> 00:23:28,400
Because that's the difference between
model 1 and model 2, right?

361
00:23:28,400 --> 00:23:32,400
It's the same thing.
Okay.

362
00:23:32,400 --> 00:23:33,750
Now let's go back to the script.

363
00:23:33,750 --> 00:23:36,910
I just want to show you the standardized
solutions.

364
00:23:36,910 --> 00:23:38,610
Again, we did this last week, but I just

365
00:23:38,610 --> 00:23:41,380
want to run through it, and show you the
confidence intervals.

366
00:23:41,380 --> 00:23:43,020
I'm going to run them all at once, because
I

367
00:23:43,020 --> 00:23:46,890
want to get through this, so we can move
on

368
00:23:46,890 --> 00:23:49,420
to the final part of this script.

369
00:23:52,580 --> 00:23:53,080
So

370
00:23:56,310 --> 00:24:00,900
here's where the, the standardized
regression analyses started.

371
00:24:00,900 --> 00:24:06,460
The thing to notice in the standardized is
the regression constant is always

372
00:24:06,460 --> 00:24:11,240
right around zero, and this is out to, you
know, 18 significant digits, right?

373
00:24:12,260 --> 00:24:13,190
oops, didn't want to do that.

374
00:24:14,940 --> 00:24:18,370
But its zero right we're standardizing it
so that the intercept is zero.

375
00:24:20,070 --> 00:24:21,457
But if you look at say,

376
00:24:21,457 --> 00:24:25,253
you know the model R squared percentage of
variance explained

377
00:24:25,253 --> 00:24:29,950
that's exactly the same and the width of
the confidence intervals.

378
00:24:29,950 --> 00:24:31,632
Those will be exactly the same.

379
00:24:31,632 --> 00:24:34,480
It's just the, the intercept that changes.

380
00:24:35,950 --> 00:24:36,450
Okay.
.

381
00:24:38,940 --> 00:24:41,960
So just to drive that point home, we

382
00:24:41,960 --> 00:24:45,980
could do that same model comparison To
compare model

383
00:24:45,980 --> 00:24:49,150
2 and model 3 standardized, but you're
going to

384
00:24:49,150 --> 00:24:51,840
get the same exact thing that you got
before.

385
00:24:51,840 --> 00:24:52,990
All right?

386
00:24:52,990 --> 00:24:53,810
So, let's run it.

387
00:24:54,840 --> 00:24:59,150
So, here's the standardized model
comparison.

388
00:24:59,150 --> 00:25:06,158
Here's the F value and the P value.
That same P value, 0.002.

389
00:25:06,158 --> 00:25:08,860
Just to show you it's exactly the same,

390
00:25:08,860 --> 00:25:12,010
I reran the unstandardized, there are the
values.

391
00:25:12,010 --> 00:25:13,080
Okay.

392
00:25:13,080 --> 00:25:15,620
The sums of squares, those change, right?

393
00:25:15,620 --> 00:25:19,880
Because they change the scale of the, of
the variables, but

394
00:25:19,880 --> 00:25:24,320
the F ratio and the P value are exactly
the same.

395
00:25:24,320 --> 00:25:24,820
Okay?

396
00:25:27,680 --> 00:25:30,090
Okay, so the last thing I want to do in

397
00:25:30,090 --> 00:25:34,760
this lab is now just radically reduce the
sample size.

398
00:25:34,760 --> 00:25:38,000
So we had a really nice sample size of
200.

399
00:25:38,000 --> 00:25:41,660
Now let's cut it down to 20, and see what

400
00:25:41,660 --> 00:25:46,790
happens to some of these significance
tests and confidence intervals.

401
00:25:46,790 --> 00:25:52,920
So I'm just going to take a random sample
of 20 from this data

402
00:25:52,920 --> 00:25:56,880
frame PE.
And I'm going to call it PE20.

403
00:25:56,880 --> 00:26:00,380
And the way to do that is to use a
function called sample.

404
00:26:00,380 --> 00:26:04,380
And I'm just going to take out 20 rows.

405
00:26:04,380 --> 00:26:06,220
I want all, I still want all the columns,
I

406
00:26:06,220 --> 00:26:10,320
want all the variables but I just want 20
subjects.

407
00:26:10,320 --> 00:26:13,380
So I'm going to do that and run the

408
00:26:13,380 --> 00:26:18,300
describe function and now I have just 20
subjects.

409
00:26:20,040 --> 00:26:25,510
and you'll notice that the standard errors
are a lot larger because there's more

410
00:26:25,510 --> 00:26:31,000
sampling error if we're only getting
samples of, of 20.

411
00:26:31,000 --> 00:26:34,960
So what happens with the correlations?

412
00:26:36,390 --> 00:26:38,540
Remember that this is just a random sample
of 20, so

413
00:26:38,540 --> 00:26:42,220
I'm not sure what the actual correlation
values will be, let's see.

414
00:26:46,006 --> 00:26:47,439
let me go back up in the output.

415
00:26:49,790 --> 00:26:51,752
pretty close to before, although now we
got a 0.2.

416
00:26:51,752 --> 00:26:53,314
so we've got 0.33, 0.38, and 0.2.

417
00:26:53,314 --> 00:26:56,754
this is randomized differently each time,
so I

418
00:26:56,754 --> 00:27:00,240
wasn't sure what I was going to get there.

419
00:27:00,240 --> 00:27:05,370
sort of nice that we got a 0.33, right,
because we had 0.33 with an n of 200.

420
00:27:05,370 --> 00:27:11,360
So let's go to that 0.33 with an n of 20,
and see if it's significant.

421
00:27:11,360 --> 00:27:14,320
So age and active years is right here.

422
00:27:15,870 --> 00:27:20,465
And there's that 0.33, and here's the
p-value.

423
00:27:20,465 --> 00:27:21,830
It's 0.16.

424
00:27:21,830 --> 00:27:27,780
So the same exact magnitude correlation,
0.33, is

425
00:27:27,780 --> 00:27:31,190
now no longer significant with an n of 20.

426
00:27:31,190 --> 00:27:33,950
That's because the standard error is
higher.

427
00:27:34,950 --> 00:27:41,040
So, again this illustrates one of the
problems with null hypothesis significance

428
00:27:41,040 --> 00:27:43,670
testing is it's biased by sample size.

429
00:27:43,670 --> 00:27:47,190
That same exact correlation coefficient
before

430
00:27:47,190 --> 00:27:49,670
we could have claimed was statistically
significant

431
00:27:49,670 --> 00:27:53,030
with an n of 200, now with an n of 20, we
can't.

432
00:27:54,340 --> 00:27:55,640
so that's a little bit funky.

433
00:27:56,774 --> 00:28:02,600
even here, a correlation of 0.38, still
not significant, the p is 0.09.

434
00:28:02,600 --> 00:28:06,448
Okay, so when you're doing correlational

435
00:28:06,448 --> 00:28:12,945
analyses or correlational studies, you
typically need large samples.

436
00:28:12,945 --> 00:28:13,505
[SOUND]

437
00:28:13,505 --> 00:28:19,480
Now let's look at what happens to the
regression models.

438
00:28:19,480 --> 00:28:22,890
So again, I'm just going to run all of
these

439
00:28:22,890 --> 00:28:25,690
at once, because I think you get the gist.

440
00:28:27,970 --> 00:28:33,850
and let me go down to model 3, model 3.20,
which has

441
00:28:33,850 --> 00:28:40,980
both predictors in the equation.
And if we go to the table of coefficients.

442
00:28:40,980 --> 00:28:44,790
What we see is, neither one of them is
significant.

443
00:28:44,790 --> 00:28:48,100
So remember that's different from before,
in when we

444
00:28:48,100 --> 00:28:51,990
had an n of 200, both of these were
significant.

445
00:28:51,990 --> 00:28:53,320
Now neither one is significant.

446
00:28:55,170 --> 00:28:59,200
Also notice the confidence intervals are
much much wider.

447
00:28:59,200 --> 00:29:00,790
Again that's because the sample size is

448
00:29:00,790 --> 00:29:04,960
so much smaller we expect more sampling
error.

449
00:29:04,960 --> 00:29:08,800
We have a higher standard error, wider
confidence intervals.

450
00:29:11,140 --> 00:29:11,410
Okay.

451
00:29:11,410 --> 00:29:14,040
Finally let's just do that model
comparison.

452
00:29:15,150 --> 00:29:17,820
And you guessed it.

453
00:29:17,820 --> 00:29:19,352
It's not going to be significant.

454
00:29:19,352 --> 00:29:23,160
So the f is really low and we have a p

455
00:29:23,160 --> 00:29:26,920
value of 0.7, so now I can't claim that
model 3

456
00:29:26,920 --> 00:29:30,820
is any better than model 2 even though it
does account

457
00:29:30,820 --> 00:29:34,820
for a little bit more of the variance in
the outcome variable.

458
00:29:34,820 --> 00:29:36,530
but again, that's because

459
00:29:36,530 --> 00:29:41,215
we don't have a sufficient sample size
with 20

460
00:29:41,215 --> 00:29:46,170
to detect statistical significance and
reject the null hypothesis.

461
00:29:46,170 --> 00:29:50,650
So again, that's, that's the importance of
sample size when doing NHST.

462
00:29:52,120 --> 00:29:55,844
and that I believe, yep, wraps it up.

463
00:29:55,844 --> 00:29:59,060
so this is all you'll need to do
assignment 5.

464
00:29:59,060 --> 00:30:01,920
Again, it'll be on correlation and
regression.

465
00:30:01,920 --> 00:30:07,060
But with an emphasis on model comparisons
and confidence intervals.

466
00:30:07,060 --> 00:30:09,260
Good luck on that, and I'll talk to you
next week.

